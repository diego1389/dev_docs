## Cloud computing:

- Routers know where to send your packages on the internet.
- Switch takes a package and send it to the correct server / client on your network.
- Scaling is limited in traditional architectures.
- Clound computing is the on demand delivery of compute power, database storage, applications and other IT resources.
- Pay as you go pricing.
- You can access as many resources as you need, almost instantly.
    - **Private cloud:** cloud services used by a single organization, not exposed to the public.
    - **Public cloud:** Azure, Google cloud and AWS.
        - Delivered over the internet.
    - **Hybrid cloud:**
        - Keep some services on premises and extend some others to the cloud.
- Five characteristics of cloud computing:
    1. On demand and self-service.
    2. Broad network access (available over the network and can be accessed by diverse client platforms).
    3. Multi-tenancy an resource pooling (multiple customers can share the same infrastructure and security with security and privacy). Multiple customers are serviced from the same physical resources.
    4. Rapid elasticity and scalability (automatically acquire and dispose resources as we need).
    5. Measured service.

- Six advantages:
    1. Trade capital expenses for operational expenses (you don't own hardware).
    2. Benefits by massive economies of scale (price reduced due to large scale).
    3. Stop guessing capacity.
    4. Increase speed and agility.
    5. Stop spending money running and maintaining data centers.
    6. Go global in minutes: leverage the AWS global infraestructure.
- Types of cloud computing:
    1. Infraestructure as a service (IaaS)
        - Building blocks for cloud IT
        - Networking, computers, data storage.
        - Amazon EC2
    2. Platform as a service (PaaS)
        - Removes the need of the organization to manage infraestructure.
        - Focus on the deployment and management of your applications.
        - Elastic Beanstack (on AWS).
    3. Software as a service
        - Organization doesn't manage anything.
        - Reckognition.
- Pricing:
    - Compute: pay for the exact compute time.
    - Storage: pay for the exact amount of data stored for the cloud.
    - Data transfer OUT the cloud (data transfer in is free).
- AWS Global Infraestructure:
    - AWS regions.
        - All around the world: Paris, Spain, etc.
        - Names: us-east1, eu-west-3, etc.
        - A cluster of data centers.
        - Most services will be linked in the scope to a specific region.
    - AWS availability zones.
        - One or more discrete data centers with redundant power, networking and connectivity.
        - ap-southeast-2a, ap-southeast-2b, ap-southeast-2c.
        - They are all connected with high bandwidth ultra low latency networking.
    - AWS Edge locations. 
        - 216 points of presence in 84 cities across 42 countries.
        - Content is delivered to end users with lower latency.
    - Region table to check availability of the service in your region.
        - Shared responsability model. Customer is responsable for the security in the cloud (configuration, etc). AWS is responsable for the security of the cloud.
        - Customer is responsible for operating system updates and patches of the instances.

    
    - **IAM:** identity access management.
        - It is a global service (doesn't need a region).
        - Since it is a global service when we create users they are for all regions.
        - Create users and assign them to group.
        - Root account created by default. 
        - One user represents one person in the organization.
        - Groups can only contain users not other groups.
        - Not all users have to belong to a group.
        - A user can belong to multiple groups.
        - User / groups can be assigned JSON documents called policies. These policies helps us to define permissions for our users.
        - IAM -> Users -> Add user -> Add access to the AWS console -> And password -> Add user to a group -> Create group -> Attach policy to the group (admins) -> Create group. 
        - -> Tags
            - Tags are a way to mark the users and add them some attributes but they don't change how AWS works.
        - -> Review and create user.
        - You can download a csv with the user information. 
        - The policy has an effect (Allow), an action (get*, list*) and a resource (*) to grant access to the users to different actions in certain resources.
        - You can create your custom policy usinga visual editor or creation the JSON file.
        - Password policy: 
            - Minimum length.
            - Require specific character types.
            - Allow users to change their passwords.
            - Require your users to change the password after some time.
            - Prevent password re use.
            - To change the policy: IAM -> Account settings -> Set password policy -> Change it and save changes.
        - Multi factor authentication (MFA device).
            - Virtual MFA device (google authenticaticator or Authy).
            - Universal 2nd factor (U2F) Security Key (USB) (Yubikey).
            - In the Dashboard of the the root user -> Security Alerts (Enable MFA) ->  Install of compatible application (Authy) -> Scan del QR code -> Write two consecutive 
    - Three ways to access AWS:
        1. AWS management console.
        2. AWS Command Line Interface (CLI).
            - Install AWS CLI on windows (download version 2 for windows).
            - Open CMD and type:
            
            ```batch
            aws --version
            ``` 
        3. AWS Software Developer Kit (SDK). For this you need to generate access keys through AWS Console.
            -  Access Key ID : user name
            - Secret Access Key : password
            - To create an access key login with an account user (not root).
                - IAM -> Users => Select user => Security Credentials Tab => Create Access Key 
                - aws configure
                    - Insert AWS Key Id.
                    - Insert AWS Secret Access Key.
                    - Default region name.
                    - Default output format (Enter).
                    ```batch
                    aws iam list-users
                    ```
            - AWS cloud shell (a terminal in the cloud inside AWS portal). 
            - IAM roles for services
                - Some AWS service will need to perform actions on your behalf (EC2 instance virtual server).
                - To do so we will assign permissions to AWS services with IAM roles.
                - EC2 instance roles.
                - Lamda function roles.
                - Roles for CloudFormation.
                IAM -> Roles -> Create Role -> AWS service -> EC2 -> Next: permissions -> IAMReadOnlyAccess 
                    -> Tags -> Review (name) => Create.
        - IAM Security tools.
            - **IAM Credentials Report:** (account-level), which is a report that lists all your account's users and the status of their various credentials.
                IAM -> Credential report ->  Download.
                - when the user created, password policies, access_key_1_active.
            - **IAM Access Advisor:** (user-level), shows the service permissions granted to a user and when those where last accessed.
                IAM -> Users -> Select a user -> Access advisor.
                    - It shows which last services where used.
        - IAM best practices:
            - Don't use root account.
            - One physical user = one aws user.
            - Assign users to groups and assign permissions to grupos (manage permissions at group level not user level).
            - Create a strong password policy.
            - Use and enforce the user of MFA.
            - Use Access Keys for programmatic access (CLI / SDK).
            - Audit permissions of your account with the IAM credentials report.
            - Never share IAM users and access keys.
        - **Shared responsability model for IAM:**
            - AWS is responsabie for:
                - Infraestructure
                - Configuration and vulnerability analysis.
                - Compliance validation.
            - You
                - Users, groups, roles policies management and monitoring.
                - Enable MFA on all accounts.
                - Rotate all your keys often.
                - Use IAM tools to apply appropiate permisssions.
                - Analyze access patterns and review permissions.
        - You can create a budget in the billing section. 
            Billing -> Create budget -> Cost budget -> Recurring -> 10 dollars monthly -> Configure threshold 
    
    - **EC2 Section:**
        - Elastic Compute Cloud, infraestructure as a service.
            - You can rent VM.
            - You can store data on virtual drives (EBS).
            - Distribute load accross machines (ELB).
            - Scale services using auto scaling group (ASG).
        -  Configuration options:
            - OS (Windows and Linux).
            - Compute power access (CPU).
            - Random access memory.
            - Storage space:
                - Network attached (EBS & EFS).
                - Hardware (EC2 Instance Store).
            - Network card.
            - Firewall rules.
            - Bootstrap script (configure at first launch).
        - EC2 -> Instances -> Launch instance -> Choose amazon AMI (template) -> Choose an instance type based on your needs -> Configure instance details -> User data text (copy script to configure a web service) -> Add storage -> Add tags -> Configure security group (firewall) (Add rule -> http -> port:80 -> from 0.0.0.0/0 (everywhere)) -> Review and launch -> Create new key pair.
        - Once finished launching it gives you a public ip address to watch the website.
        - Once you stop your instance you don't have to pay for it.
        - You can also terminate your instance (delete it).
        - If you stop and start the instance you get a new public ip.
        - Instance types: https://aws.amazon.com/ec2/instance-types
            - m5.2large:
                - m: instance class
                - 5: generation (AWS improves them over time).
                - 2xlarge: size within the instance class.
            - Different types:
             1. **General purpose:** 
                - Balance between compute, memory and networking.
             2. **Compute optimized:** 
                - High performance processors. (C name)
                - For batch processing workloads, media transcoding, machine learning, gaming servers, etc.
             3. **Memory optimized:**
                - R name
                - Fast performance for workloads that process large data sets in memory.
                -   Floating point number calculations, data pattern matching, graphic processing.
             4. **Storage optimized:**
                - Name i, h, d.
                - Online transaction processing.
                - Relational and NoSQL.
                - Data warehousing applications.
                - Distributed file systems.

        - Security groups:
            - They control how traffic is allowd into or out of the EC2 instances.
            - They only contain allow rules.
            - Can reference by IP or by security group.
            - They act as a firewall on EC2 instances.
            - Are going to regulate access to ports, authorized ip ranges, control the inbound network (from the other to the instance) and outbound network (from the instance to the other):
                - SSH port 22. (Log into a Linux instance)
                - FTP port 21. (upload files into a file share)
                - SFTP port 22. (upload files using SSH)
                - HTTP port 80 (access unsecured websites). 
                - HTTPS port 443 (access secured websites).
                - RDP port 3389 (Remote Desktop Protocol, log into a Windows instance).
            - You can go to security groups and edit inbound and outbound rules.
            - You can reuse security groups in multiple instances.
            - Anywhere IP v4: 0.0.0.0/0. Anywhere IP v6: ::/0.
    - Connect to our instance using **ssh**:
    - Use the Public IP to connect using SSH.
    - Use the key file (EC2Tutorial.pem) to get into the machine.
    - chmod 0400 for the key file (only read permissions). 
    - To access the EC2 instance via CLI:
    ```batch
     ssh -i .\EC2Tutorial.pem ec2-user@18.223.114.120
    ```
    - For windows versions < 10 you need to use Putty instead of SSH.
    - The user is always ec2-user.
    - For windows the file must have full access and only your user should have access to it (disable inheritance).
    - **EC2 instance connect:** Instances -> Select instance -> Connect -> opens an ssh terminal in the browser (aws will automatically upload the key to the instance).
    - This doesn't work if you block the ssh port.
    - **Amazon instance roles:**
        - Don't use your AWS credentials on the instance to execute AWS commands, instead use instance roles.
        - Attach role to instance: Select the instance => Security => Modify IAM role => DemoRoleForEC2.
        ```batch
        aws iam list-users
        ```
    - **EC2 Instance Launch types**
        - EC2 On demand:
            - Billing per second after the first minute.
            - Has the hightest cost but not upfront payment.
            - No long-term commitment.
            - For short term and un-interrupted workloads.
        - EC2 Reserved instance (minimum of 1 year, 1 or three not between 1 and three).
            - Up to 75 % discount compared to on-demand.
            - Reserve a specific instance type.
            - Purchasing options: All upfront (discount), partial upfront, no upfront. 
            - Steady-stage usage applications (databases).
        - Convertible reserved (long workloads).
            - Can change the EC2 instance type.
            - Up to 54 % discount (a bit less than reserved instance).
        - Scheduled reserved instances.
            - Specific time window (fraction of day).
            - Still need 1 or 3 years conmitment.
        - EC2 Spot instances
            - Discount of 90 % compared to On-demand but you can lose them if your max price is less than the current spot price.
            - Not for critical job.
            - For workloads that are resilient to failure (batch jobs, image proccessing). 
        - Dedicated hosts. 
            - Physical server in a data center fully dedicated to you.
            - **Compliance requirements** and reduce costs by allowing you to use your **existing server-bound software licenses**. 
            - Allocated for your account for a 3 year period reservation.
            - Software that has complicated licensing model.
            - More expensive. 
            - Per host billing. 
        - Dedicated instances (hardware dedicated to you, you don't get access to the hardware).
            - May share hardware with other instances in the same account.
            - No control over instance placement.
            - Per instance billing.
    - **EBS Volume (Elastic Block Store)**
        - Network drive that you can attach to your instances while they run.
        - Persist data even after the instance termination.
        - Can only be mounted to one instance at a time.
        - Bound to a specific availability zone.
        - Network USB stick.
        - 30 GB of free EBS storage of type gp2 per month.
        - It's not a physical drive.
        - Can be detached from an EC2 instance and atached to another one quickly.
        - It's locked to an Availability Zone (AZ).
        - To move volume across you first need to snapshot it.
        - Have a provisioned capacity in advance (size in GBs).
        - You can increase capacity over time.
        - Can have more than one EBS attached to the same instance.
        - It is possible to create an EBS volume and leave them unatached.
        - Instance details root device and block devices (EBS volumes).
        - A volume is created when you create your EC2 instance.
        - EC2 -> Elastic Block Store -> Volumes -> Create volume -> gp2 (General purpose)-> change size -> availability zone must be in the same zone as your instance -> create volume.
        - Select the volume -> actions -> Attach volume -> select your instance. 
        - To check it Go to instances -> select your instance -> Select the Storage tab.
        - ESB volumes that do not have delete in termination set to true will persist even if you delete the instance.
        - **EBS snapshots:** you can make snapshots of your volumes (backup) and copy them across AZ or regions.
            - Select Volume -> Actions -> Create snapshot
            - To verify the goto to EBS -> Snapshots (under volumes).
            - The snapshot is available in your region, not to specific AZ (availability zone).
            - You can create a volume from a snapshot
                - Select snapshot -> Actions -> Create volume. You can configure it in a different availability zone.
    - **AMI (Amazon Machine Image)**
        - A customization of an EC2 instance.
        - It can be built for an specific region.
        - You can add your own software, configuration, operation system, monitoring, etc.
        - You have to make it and mantain it yourself or launch (and buy) from AWS marketplace.
        - AMI process:
            - Start an EC2 instance and customize it.
            - Stop the instance (for data integrity).
            - Build an AMI.
            - Launch instances from other AMIs.
        - Right click in the instance you want -> Image and templates -> Create image -> Type name (it will create a snapshot of the root volume) -> Create
        - Now in Launch instance it will appear under the MyAMIs tab (when you are creating the instance).
        - If you create a new instance for your AMI you will have the web server configured without needing to paste the configuration script (but it will have the same private ip address).
        - **EC2 image builder:** automate the creation, mantain, validate and test EC2 AMI. It is a regional service.
            -  Can be run scheduled.
            - Go to EC2 image builder -> create image pipeline -> Name it -> build schedule -> Create a new recipe -> Output type: AMI -> Name the recipe and add version (1.0.0 f.e) -> Select image name Amazon Linux 2 x86 -> Add components (Coreto 11 for java and aws cli v2) -> Create new infrastructure -> Create new role* -> IAM Role: EC2InstanceProfileForImageBuilder -> Instance type: t2.micro -> Next -> Create distribution settings -> Create pipeline.
            
            - \*Create new role -> EC2 -> Permissions (EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds and AmazonSSMManagedInstanceCore) -> Review -> role name: EC2InstanceProfileForImageBuilder
            
            - Go to pipeline (to the demo we just created) -> Action : run pipeline -> Check in output image (in pending for now) and click in the version hyperlink -> It changes state to testing.
            - In instances (EC2 service) we have a new Build instance for MyDemoRecipe created by EC2 image builder. It terminates it after a while.
            - After everything finishes it add a new AMI that you can use to create a new instance with its configuration.
            - Launch a new instance using the new AMI, connect using EC2 Connect (Connect -> EC2 instance connect) and check java --version and aws --version.
    - **Local EC2 instance store**
        - If you need high performance hardware disk use EC2 instance store.
        - Better IO performance.
        - They loose their storage when they're stopped.
        - Risk of data loss if hardware fails.
        - Good for buffer / cache / scratch data or temporary data.
    - **EFS Elastic File System**
        - Managed NFS (network file system) that can be mounted in hundreds of EC2 instances at a time.
        - IT works with Linux EC2 instances in multi-AZ.
        - HIghly available, scalable, expensive, pay per use, no capacity planning.
        - EBS can be attached to only one instance at a time (to move it we need a snapshot, a copy).
        - EFS is shared accross multiple instances in multiple Availability Zones.
    - **Shared responsability for EC2 storage:**
        - AWS: infrastructure, replication for data for EBS volumes and EFS drives, replacing faulty hardware and employees cannot access data.
        - Client: backups, snapshot procedures, setting up encryption, responsability of any data on the drives, understanding the risk of using EC2 instance store.
## Elastic load balancing and auto scaling groups: 

- Scalability means that an application / systema can handle greater loads by adapting.
- **Vertical scalability:** 
     - Increasing the size of the instance. Change from t2.micro to t2.large. 
     - Common for non distributed systems (such a database).
- **Horizontal scalability (elasticity):** 
    - Increasing the number of instances.
    - Implies distributed systems. 
    - Common for web applications.
    - Auto scaling group.
    - Load balancer
- **High availability:**
    - At least 2 availability zones. 
    - Auto Scaling Group multi AZ.
    - Load balancer multi AZ.
 - **Scalability:** ability to accomodate a larger load by making the hardware stronger (scale up) or by adding nodes (scale out).
 - **Elasticity:** there will be some "auto-scaling" so that the system can scale based on the load. This is "cloud-friendly": pay-per-use, match demand, optimize costs.
 - **Agility:** new IT resources are only a click away. 
- **Load balancer**
    - Servers that forward internet traffic to multiple servers (EC2 instances) downstream.
    - Spread load accross multiple instances.
    - Expose a single point of access (DNS) to your application.
    - Seamlessly handle failures of downstream instances.
    - Do regular health checks to your instances.
    - Provide SSL termination (HTTPS) for your websites.
    - High availability across multiple AZ.
    - Amn ELB (elastic load balancer) is a managed load balancer.
        - AWS guarantees that it will be working.
        - AWS provides only a few configuration knobs. 
        - Provide SSL termination (HTTPS).
		- Highly available across AZ.
		- 3 kinds of load balancers:
			1. **Application load balancer (layer 7):** HTTP and HTTPS.
			2. **Network load balancer (Layer 4):** ultra high performance, allows for TCP. 
		    3. **Classic load balancer (Layer 4 and 7)**
    - **Hands-on load balancer:**
	    1. Create two new EC2 instances:
            - **First instance details:** Subnet: us-east-2a -> copy the initialization script on user data -> Next add storage (leave it as is) -> Add tag (Name : First server) -> Security group launch wizard 3 (SSH all and TCP all ipv4 and v6) -> Review and Launch.
            - **Second instance details:** Subnet: us-east-2b -> copy the initialization script on user data -> Next add storage (leave it as is) -> Add tag (Name : Second server) -> Security group launch wizard 3 (SSH all and TCP all ipv4 and v6) -> Review and Launch.
        2. Go to Load balancing -> Load balancers -> Create load balancer -> Create application load balancer -> Add name -> internet facing -> ipv4 -> protocol http -> AZ: mark a, b and c -> Create a new security group (80 all) -> 
        3. Configure routing: 
            - Create target group: Add name -> target type: instance -> http port 80 -> Add the two instances from step one to registered targes list -> Create
        4. Wait a few minutes until the load balancer is ready (active) -> copy the DNS name (Ip address) and paste in a browser -> Refresh the browser and check how it balances the traffic between the two instances. 
        5. Stop one of the instances. Since it moves to an unhealthy stata in the registered targets it will forward the traffic only to the remaining healthy instance. 
    - **Autoscaling group:**
        - Scale out to match an increase load or scale in to decrease instances.
        - Automatically register new instances to a load balancer.
        - Replace unhealthy instances.
        - Costs savings.
    - **Auto scaling group hands-on:**
        *  Go to auto scaling group -> Create Amazon auto scaling group -> \* Create a template -> Select the created template -> Next, subnets: select all the AZ -> Next -> Attach existing load balancer -> Select target group -> ELB healthcheck (if it detects my instance is unhealthy it will be replaced) -> \**Configure size -> Desired: 2, Minimum: 1, Max: 4 -> None -> next, next Create auto scaling group. -> Check the instance management tab and you can see the instances that were created automatically -> YOU can also check it on the Targets groups, the instances tab and the load balancer.
        - Terminate one instance and check in the activite tab on Auto scaling group that it adds a new instance because the desired capacity is two. 
        
        \* Create a launch template -> Name it -> AMI (Amazon Linux 2) -> t2.micro -> Choose key pair -> VPC -> Security group launch-wizard-1 -> Scroll down to the details and add the user data script -> Create launch template.

        \** If you want to do it automatically add a target tracking policy -> Select a metric (CPU) -> etc
        * We just created a template for instances creation to use it in our autoscaling group.
        
## S3:

- Infinitely scaling storage.
- Many websites use Amazon S3 as a backbone.
- Backup and storage.
- Disaster recovery.
- Archive data.
- Hybrid cloud storage.
- Media hosting.
- Data lakes and big data analytics.
- Static website.
- **Buckets (directories):**
    - Buckets most have a globally unique name (across all regions all accounts).
    - Buckets are defined at the region level.
    - S3 looks like a global service but buckets are created in a region.
    - Naming convention:
        - No uppercase
        - No underscore
        - Not IP, etc
    - **Objects (files):**
        - Objects have a key.
        - The key is the full path to the object:
            - s3://my-bucket/my_file.txt
            - s3://my-bucket/my_folder/another_folder/my_file.txt
            - The key is composed of a prefix (directories) + object name (file name).
            - There is no concept of directories within buckets.
            - Object values are the content of the body.
            - Max size is 5TB (5000 GB).
                - If using more you must use "multi-part upload".
            - Metadata (key / value pairs).
            - Tags
            - Version iD.
    - **Hands-on S3:**
        - S3 -> Buckets -> Create bucket -> Bucket name (unique) -> select region -> Create bucket.
        - Upload -> Add files -> Select file -> Upload. Now the object appears in the bucket.
        - Go to the file object review -> Object actions -> Open (presigned url with my AWS credentials).
        - Click on object url (access denied because its not public)
        - Create a folder inside the bucket (images).
        - Delete folder (deletes objects inside it). 
      - **S3 security:**
        - **User based:** IAM policies: which API calls should be allowed for a specific user from IAM console.
            - Allow IAM user to access our buckets
        - **Resource based:** S3 Bucket policy, a rule attached directly into your buckets.
            - Allows public access.
        - **Object Access Control List (ACL)**
        - **Bucket Access Control List (ACL)**
        - Encryption.
        - You can create an EC2 Instance role to grant access to the S3 objects (IAM permissions) and then attach that role to the instance. (There is no IAM user for an EC2 instance, bad!).
        - Cross account access (other AWS account) -> Create a bucket policy.
        - S3 bucket policies:
            - JSON based policies.
                - Actions: set of ASPI to allow or deny (fe: s3.GetObject)
                - Effect: Allow, Deny
                - Principal: The account or user to apply the policy to.
            - Use the S3 bucket policy to grant public access to the bucket.
            - Force objects to be encrypted at upload.
            - Grant access to another account.
            - Disable bucket settings for block public access (to prevent data leaks).
        - **Hands on security:**
            - Go to the **Bucket** permissions -> Block public access Edit -> Edit -> Uncheck everything and save -> write confirm -> Edic bucket policy -> Generate policy\* and paste it under Policy -> Save changes.

            \*Edit bucket policy -> Policy generator -> S3 Bucket policy -> Allow -> Principal: * -> Action: Get object -> Amazon resource name: arn:aws:s3:::diego-ccp-demo-2021/* -> Add statement -> Generate policy -> copy the JSON -> Paste it under Policy (Edit bucket policy).
    - **Websites:**
        - Host static websites under awz.
        - <bucket_name>.s3-website.<AWS-region>.amazonaws.com
        - **Website hands-on**:
            - Upload beach.jpg and index.html on the public bucket. -> Bucket properties -> Static website hosting -> Edit -> Host static document -> Index document (index.html) -> Save -> Now under static website hosting there is a bucket website endopoint.
        - **S3 Versioning overview:**
            - You can version your files in Amazon S3.
            - It is enabled at bucket level.
            - It protects against unintended deletes (ability to restore a version).
            - Any file that is not versioned prior to enabling versioning will have version null. 
        - **Hands-on:**
            - Go to bucket -> Bucket versioning -> Enable -> Save
            - Edit the index.html file -> Upload it again -> Go to the file versions tab. 
            - TO check the versions under the Buckets -> objects list check the List versions option. Then you can delete the version using its id.
            - When you delete an object that doesnt have version it (but we need to activate versioning first) adds a delete marker. If you check the List versions you can restore it. If you delete the delete market it restores the object. 
      - **S3 Access Logging:**
        - For audit purpose.
        - Any reqeust made to S3 will be logged into another S3 bucket.
            - **Hands-on:**
                - Create new bucket (for the logs of the other bucket).
                - On the other bucket go to properties -> Server access logging -> Edit -> Enable -> enable -> target bucket/logs -> save
                - After a couple of hours you will get the logs into the logging bucket.
    - **S3 Replication:**
        - Replicate content from one bucket into another.
        - Must enable versioning in source and destination
        - Cross Region Replication (CRR).
        - Same Region Replication (SRR).
        - Buckets can be in different accounts.
            
            - **Hands-on:**

            - Create new bucket -> select different region -> enable versioning.
            - Go to main bucket -> Management -> Replication rules -> Create replication rule -> Destination -> add bucket name -> Choose IAM role -> Create new role -> Save
            - Objects added before replication are not replicated. 

    - **S3 Storage classes:**
        - General class (99.99% availability).
            - Frequently accessed data.
            - Low latency.
            - Sustain 2 concurrent facility failure
        -  Standar-Infrequent Access (IA)
            - Rapid access.
            - 99.9% availability.
            - Lower cost compared to standard.
            - As data store for distaster recovery, backups.
        - Standard-Infrequent Access
        - One zone-infrequent Access:
            - Single AZ.
            - 99.%
            - Lower cost comparted to S3-IA
        - Intelligent Tiering
            - 99.9% availability.
            - Cost-optimized by automatically moving objects between two access tiers based on changing patters:
                - Frequent access
                - Infrequent access.
        - Glacier
        - Glacier deep archive
            - Low cost object storage
            - Data is retained for longet term(years)
            - ASmazon Glacier (cheap):
                - Expedited (1 to 3 minutes retrieval)
                - Standard (3 to 5 hours)
                - BUlk (many files 5 to 12 hours)
        - Reduced Redundancy Sotrage (deprecated)
        - Moving between storage classes.
        - Moving objects can be automated using lifecycle configuration.
        - **Hands-on:**
            - Upload files -> Storage class -> Standard IA -> Edit it a move it to Glacier (Select the file and scroll down to storage class).
            - When you edit the storage class it creates a new version of the object.
            - Go to bucket -> Management -> Create Lifecycle rules -> Name it -> Apply to all objects in the bucket -> transition current versions of objects between storage classes -> 

        - Durability: how often you will use a file (99.9999999%)
        - Availability: measures how readily available a service is (99.99%, 53 minutes a year will not be available). Varies depending of the storage class. Transition to Standard-IA after 35 days -> To glacier after 180 days -> Expire current versions of objects (check) -> Expire current versions of objects after (n) days.
      - **S3 Object Lock & Glacier vault lock:**
        - Object Lock (WORM write once read many).
            - BLock an object version deletion for a specified amount of time.
        - Glacier Vault Lock
            - Lock the policy for future  edits (cannot longer be changed).
            - Compliance and data retention.
     - **Shared responsability:**
        - AWS: responsible for infraestruction, durability availability, sustain concurrent loss of dataa, configuration and vulnerability analysys.
        - User: versioning, bucket policies, replication setup, logging and monitoring, storage classes, data encryption.
       - **Snow Family Overview**
        - Highly secure portable (offline) devices to collect and process data at the edge and migrate data into and out of AWS.
        - Data migration:
            - Snowcone
                - Smaller.
                - 8 TB
            - Snowball edge
                - Pay per data transfer job.
                - Block storage and Amazon S3 compatible object storage.
                - Storage optimized or compute optimized.
                - 80 TB
            - Snowmobile
                - It is a truck.
                - 100 > PB
        - Edge computing
            - Snowcone
            - Snowball eedge
        - It is a physical device that AWS will send to you to upload a lot of information to AWS.
        - It process data that is being created on an edge location. A truck on the road, a ship on the sea, a mine.
        - OpsHub
            - Normally the snow family uses a CLI but OpsHub is a software that you use in your computer to manage the snow family.
            - A graphical interface.
        - **Hands-on snow family:**
            - Services -> AWS Snow family -> Order one -> Import into Amazon S3
        - **Hybrid Cloud for storage:**
            -  AWS storage gateway: expose the S3 data on-premise.
            - Differnet types:
                - File gateway.
                - Volume gateway.
                - Tape gateway.

## Database intro:
- No SQL databases allow horizontal scaling (scale out). Relational databases allow only vertical scaling (more resources).
- AWS db:
    - Quick provisioning.
    - High availability/
    - Vertical and horizontal scaling.
    - Automated backup & restore operations.
    - OS patching (AWS responsability).
- **Relational database:**
    - Managed database service for DB use
        - Postgres
        - MySQL
        - MariaDB
        - Oracle
        - Microsoft SQL Server
        - Aurora (AWS propietary)
    - RDS instead of installing our own database on an EC2 instance:
        - Automated provisioning, OS patching.
        - COntinous backups and restore to a specific timestamp
        - Monitoring dashboards.
        - Read replicas for improved read performance.
        - Multi AZ.
        - We cannot SSH to our DB instance.
    - It uses an Elastic Load Balancer
    - **RDS deployments:**
        - Read replicas (up to 15).
            - Data is only written through the main DB.
        -Multi AZ: high availability.
            - Read and write through the same DB.
            - Failover DB.
        - Multi-region
            - Read replicas on multiple regions.
            - Write happens only in the main db (in one region).
    - **Amazon ElastiCache:
        - To get managed Redis or Memcached.
        - In memory databases with high performance and low latency.
        - Reduce load off databases for read intensive workloads.
        -  
    - **Amazon Aurora:**
        - Amazon propietary.
        - Supports Postgres SQL and MySQL.
        - It is cloud optimized (5x performance improvement over MySQL running on RDS and 3x Postgres).
        - It grows automatically in increments of 10 GB.
        - 20 % more expensive.
    - **Dynamo DB:**
        - Fully managed highly available with replication in 3 AZ.
        - NoSQL database.
        - Distributed serverless database.
        - Fast and consistent performance.
        - Low latency.
        - Dynamic Database Acelator (DAX).
            - In memory cache for Dynamo DB.
    - **Redshift:**
        - It's based on PostgreSQL for Online Analytical Processing (OLAP).
        - Warehousing.
        - Load data every hour (not every second).
        - Columnar storage of data (instead of rows).
        - Pay as you go.
        - BI tools as AWS Quicksight or Tableau.
    - **EMR:**
        - Elastic map reduce.
        - Hadoop clusters (big data) to analyze vast amounts of data.
        - Cluster of hundreds of EC2 instances.
        - Machine learning, web indexing.
    - **Athena:**
        - Serverless database.
        - Used to query data on S3.
        - Pay per query.
        - Output results back to S3.
    - **Amazon Quecksight:**    
        - Serverless machine learning-powered business intelligence service to create interactive dashboards.
        - Business analytics.
        - Building visualizations.
        - Perform ad hoc analysis.
    - **DocumentDB:**
        - The same as MongoDB.
        - Query and index json data.
        - 3 availabilty zones.
        - Millions of requests per sercond.
    - **Amazon Neptune:**
        - Graph database.
        - Graph dataset (social network).
        - 3 AZ. HIghly connected datasets.
        - Billions of relations.
        - Great knowledge graphs (wikipedia, social networks).
    - **Amazon QLDB:**
        - Quantum ledger db: 
        - A record of financial transactions.
        - To review history of all the changes made to your application data over time.
        - Innmutable system. Crypthographical verifiable.
        - The difference between QLDB and blockchain is that there is no descentrailization concept.
    - **Managed blockchain:**
        - Descentralization concept.
        - Join public blockchain networks.
        - Create your own scalable private network.
    - **DMS:**
        - Database migration service. Move data from one db to another.
        - Source database remain available.
    - **Glue:**
        - ETL service (Extract, transform load).
        - Fully serverless service.
- **ECS:**
    - Docker: softwere development platform to deploy apps.
        -  Apps are packed in containers (regardless of OS)
        - Any time, no compatibility issues, easier to mantain and deploy.
        - Docker repositories.
        - Docker is a sort of virtualization technology. Resources are shared with the host, many containers on one server.
    - ECS: Elastic container service. 
        - Launch the docker containers on AWS.
        - You must provision and mantain the infraestructure.
        - AWS takes care of starting of stopping the containers.
        - Has integration with an application load balancer.
    - Fargate:
        - We dont need to provision any infraestructure (dont need an EC2 instance, serverless).
        - 
    - Elastic container registry (ECR):
        - Private docker registry on AWS so they can be run on Fargate or ECS.
- **Serverless:**
    - The developers don't manage the servers anymore.
    - Function as a service.
    - **Lambda:**
        - Virtual functions:
            - Limited by time.
            - Run on demand.
            - Scaling is automated.
            - Pay per request.
            - Event-driven
            - Integrated with many programming languages.
            - Lambda container image: it must implement the Lambda Runtime API.
            - For example when you upload an image on AWS S3 it will trigger a lambda function that will create thumbnail on S3 or write metadata on DynamoDB.
            - You pay per calles (first one million requests are free).
            - Pay for the duration (in increments of 100 ms)
    - **Lambda hands-on:**
        - Lambda explanation: https://us-east-2.console.aws.amazon.com/lambda/home?region=us-east-2#/begin
        - Create function -> Use blueprint -> hello-world-python -> name -> Create function -> Test -> Create demo test -> name -> Test.
        - To run functions without provisioning servers.
        - You can edit settings and set the memory limit for your function, also time-out, 
- **API Gateway:**
    - Fully managed service for developers to easily create, publish, mantain, monitor and secure API.
    - Serverless and scalable.
- **AWS Batch:**
    - Allows batch processing, hundreds of thousands of computing batch jobs on AWS.
    - A job that has a start and an end.
    - Will dynamically launch EC2 instances or spot instances..
    - Are defined as a docker image.
    - **Lambda:**
        - Time limit.
        - Limited runtimes.
        - Limited temporary disk space.
    - **Batch:**
        - No time limit.
        - Any runtime as long as it's packed as a Docker image.
        - Rely on EBS and EC2 (not serverless).
- **Lightsail:**
    - Virtual servers, storage, databases and networking.
    - Low and predictable pricing.
    - Simpler than EC2, RD2, ELB, EBS, Route S3.
    - Deploy simple websites (LAMP, MEAN, Joomla, etc).
- **Lightsail hands-on:**
    - Select a blueprint (Wordpress website, etc), region, OS.
    - Quickly up and running.
## Deploy and manage infraestructure 
- **Cloudformation:**
    - Declarative way of outlining your AWS infraestructure for any resource.
    - Cloudformation template (EC2, security group, storage, etc).
    - Infraestructure as code.
    - No resource manually created.
    - You can estimate the costs using the template.
    - Declarative programming.
    - Ability to create and destroy infraestructure on the fly.
    - Useful when you need to repeat an architecture on different environments.
    - **Hands-on:**
        - Cloudformation -> Create stack -> Template is ready -> us-east-1 -> Upload -> 0-just-ec2.yaml -> view in designer -> Next -> Name it -> Create.
        - You can check the events that are happening.
        - You can check the instance on EC2 instances.
        - Go to CloudFormation -> Update -> Change the script to 1-ec2...
        -> Now we have a changes set -> Update stack.
- **Elastic Beanstalk:**
    - A developer centric view of deploying an application on AWS.
    - We still have full control over the configuration.
    - BeanstalkÑ Platform as a Service (PaaS).
    - Managed service.
        - Deployment strategy is configurable but performed by Elastic Beanstalk.
        - Load balancing and auto-scaling.
    - Just the application code is the responsability of the developer. 
    - **3 architecture models:**
        - Single instance deployement (for devs).
        - LB (load balancer) + AGS (auto-scaling group) great for prod and pre prod web applications.
        - AGS only: for non-web apps in production (workers).
    - Support a lot of platforms.
    - Health monitoring.
    - **Beanstalk hands-on:**
        - Elastic Beanstalk -> Create application -> Name it -> Select platform (node.js) -> Sample application -> Create application.
        - It uses Cludformation behing the scenes.
        - Copy and paste the link of the application once it finishes.
        - We can create multiple environmnent within our application.
    - **CodeDeploy:**
        - More permisive. Independent from cloud formation.
        - Deploy application automatically.
        - Works with on-premises servers.
        - Hybrid service
    - **CodeCommit:**
        - AWS version of GitHub.
        - Version control repository.
        - Private, secured and integrated with AWS.
    - **CodeBuild:**
        - Compiles source code, run tests and produces packages that are ready to be deployed.
        - Code building service in the cloud.
        - Pay-as-you-go.
    - **CodePipeline:**
        - Orchestrate the different steps to have the code automatically pushed to production.
        - Basis for CI/CD.
        - Fully managed and compatible with a lot of services.
    - **AWS CodeArtifact:**
        - Storing and retrieving dependencies (software packages).
        - WOrks with commong dependency management tools (npm, yarn, etc).
        - Developers and CodeBuild can retrieve dependencies from CodeArtifact.
    - **AWS CodeStar:**
        - Unified UI to easily manage software development activities in one place.
        -**Hands-on:**
            - CodeStar -> Create prooject -> Python (on Elastic Beanstalk) -> Repo Code commit -> t2.micro -> Select key pair -> Create Project.
            - Pipeline
            - Go to IDE -> AWS cloud 9 -> t2.micro -> shutdown after 30 minutes -> Create environmnent -> Go to IDE -> Open IDE -> You can change the code on the cloud and commit to the repo and it will trigger the pipeline -> View application.
            - Delete IDE env and the project (in overview).
    -  **AWS Code9:**
        - A Cloud IDE.
        - Used within a webbrowser.
        - Code collaboration in real-time (pair programming).
    - **AWS Systems Manager(SSM):**
        - Helps you manage your EC2 and On-Premises systemas at scale.
        - Operational insights about the state of your infraestructure.
        - Patching automation.
        - Run commands accros your entire fleet of serveers.
        - Parameter store configuration.
        - Works for windows and linux OS.
        - Install SSM agent on the systems.
    - **AWS OpsWorks:**
        - Chef & Puppet help you perform server configuration automatically.
        - They work great with EC2 and On-Premises VM.
        - An alternative to AWS SSM.
        - Only provision standard AAWS resources.

## Leveragint the AWS Global Infraestructure

- Application deployed in multiple geographies.
- Crecreased latency.
- Deploy your applications closer to your users.
- Disaster Recovery (DR).  
- Attack protection.
- **Route 53>**
    - Great to route users to the closes deployement with least latency.
    - Managed DNS.
     - Routing policies:
        - Simple RP:
            - No health checks.
        - Weighted routng policy:
            - Distribute the traffic accross multiple EC2 instancs.
            - Health ckecks.
        - Latency routing policy:
            - Minimize the latency.
            - Connect to the closes instance.
        - Failover routing policy:
            - Disaster recovery.
    - Instances -> Create instance -> 
     You need to create a domain and add different instances in different regions and AZ and add it to the Route 53 and it will redirect to what's closest to you.
- **GLobal Content Delivery Network:**
    - **Cloudfront:**
    - Edge locations.
    - Cache common requests.
    - Improves the read performances, content is cached at the edge.
    - Improves users experience.
    - 216 Point of presence globally.
    - For distributing files (S3 and caching them at the edge).
    - CloudFront can be used as an ingress (to upload fiels to S3).
    - Custom Origin (HTTP) Application load balance, Ec2 instance, S3 website.
    - If it doesn;t have it on the cage it goes to the origin.
    - CloudFront vs S3 Cross region replication:
        - CloudFront: 
            - Global edge network.
            - Files are cached for a TTL.
            - Great for static content that must be available everywhere.
            - **Hands-on:**
                - Go to S3 and create a bucket -> Name it -> Upload the website files (code/s3/files).
                - Go to cloudfront -> Create distribution -> origin domain name (s3) -> Restrict bucket access yes -> Grant Read Permissions on Bucket : Yes -> Create distribution -> Distributions -> Copy the origin domain and paste in a browser it will give you an access denied error  -> go to distribution, click on your distribution and go to origins and origin group -> edit -> copy the s3 bucket region and paste it on origin domain name (f.e: demo-diego-ccp-cloudfront.s3-us-east-2.amazonaws.com) -> Restrict bucket access -> use identity -> yes update bucket policy -> go to dvr0nrd0sc384.cloudfront.net/index.html.

        - S3 Cross region replication:
            - Must be setup for each region you want replication to happen.
            - Files are updated in near real-time.
            - Read only.
            Great for dynamic content that needs to be available at low-latency in few regions.
- **S3 Transfer Acceleration:**
    - Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region.
    - Amazon S3 transfer acceleartion speed comparison toolk.
    - Upload and downloads of data into S3.
- **AWS Global accelerator:**
    - Improve global application (availability and performance) using AWS global network.
    - Anycast IP.
    - Not CDN (cached served at the edge).
    - No caching. Improves performance for a wide range of applications over TCP or UDP (not only static websites).
- **AWS Outposts:**
    - Hybrid cloud.
    - Offer the same AWS infrastructure on-premises just as in the cloud.
    - AWS will setup and manage "Outposts racks" within your on-premises infrastructure and you can start leveragin AWS services on-premises.
    - You are responsible for the physical security.


## Cloud integration
- Multiple applications will inevitably need to communicate with one another.
- Patterns of application communication:
    - Synchronous communications.
    - Asynchronous / event based.
- Asynchronous to decouple your applications:
    - SQS queue model.
        - Fully managed service (serverless). Used to decouple applications.
        - 4 days, maximum 14 days of messages retention.
        - Messages are deleted after they-re read by consumers.
        - **Hands-on:**
            - Go to SQS -> Create queue -> Standard -> Create -> Send and receive messages -> Send message -> Poll messages -> Remove messages from the queue.
    - SNS pub/sub model.
        - SNS topic send notification tto email notification, fraud service, shipping service and SQL queue.
        - Simple Notification Service.
        - Event publishers and subscribers.
        - **Hands-on:**
            - Amazon SNS -> add topic name -> Next -> Create topic -> Create subscription -> Email -> diego-ccp-demo@mailinator.com -> Go to mailinator and add the email -> Confirm the subscription -> Now the subscription will change from pending confirmation to confirmed.
            - Topics -> demo-sns -> Publish message -> Add subject and message and send-> check mailinator inbox.
    - **Kinesis (real-time data streaming model):**
        - Real time big data streaming.
        - Kinesis data streams: low latency streaming from hundreds of thousands of sources.
        - Kinesis data firehose: load streams into S3, redshift, elasticsearch, etc.
        - Kinesis data analytics: real-time analysis on streams using SQL.
        - Kinesis video stream: monitor real-time video streams for analytics or ML.
    - **Amazon MQ):** 
        - Open protocols (MQTT, AMQP, STOMP).
        - Protocols on-premises: Apache ActiveMQ.
        - Not serverless.
        - Has queue feature and topic features.
    
## Cloud monitoring:
- **CloudWatch metrics:**
    - Billong metric (per region).
    - CPU utilization, status echecks, network (not RAM).
    - Every 5 minutes.
    - EBS volumes.
    - S3 buckets.
    - Custom metrics.
    - Alarms and alarms actions (autoscaling).
    - SNS notifications.
    - **Hands-on:**
        - Cloudwatch -> Metrics.
        - You can create an alarm during on the EC2 instance panel. You can take actions like rebooting the instance or recovery after some events (5 minutes of CPU beyond 96 % for example).
- **Cloudwatch logs:**
    - From elasticbeanstalk, ECS, lambda, cloudtrail.
    - Log agents on EC2 machines or on-premises servers.
    -  You need to run a CloudWatch agent on EC2 to push the log files you want.
    - Make sure IAM permissions are correct.
    - **Hands-on:**
        - Go to cloudwatch -> logs -> view log groups -> lambda logs->  Log streams -> Log events -> it will have the print messages from the lambda function.
- **Cloudwatch events:**
    - Schedule Cron jobs
    - Event pattern (rules to react to a service doing something).
    - EventBridge: the next evolution of CloudWatch events.
    - Default event bus: generated by AWS services.
    - Receive events from SaaS service (Zenddesk, datadog).
    - Schema registry.
    - **Hands-on:**
        - Cloudwatch -> Events -> Create rule -> Scheduled -> Add target -> trigger lamda function every certain minuts (Serverless cron job) 
- **Cloudtrail:**
    - Provides governance, compliance and audit for your AWS account.
    - Enabled by default
    - History of events / API calls made within your AWS account.
    - A trail can be applied to All regions.
    - Cloudtrail console.
    - Cloudtrail events:
        - Management events:
            - Operations that are performed on resources in your AWS account.
            - Separate Read events from Write events.
        - Data events:
            - By default are not logged.
            - Amazon S3.
            - AWS lambda function execution activity.
        - Cloudtrail insights:
            - To detect inusual activity on your account.
            - Service limits
            - Bursts of IAM actions.
            - Analyzes normal management events to create a baseline.
            - Has 90 days retention.
    - **Hands-on:**
        - Create trail -> Event history (filter by eventname).
        - You can query your Cloudtrail events with Athena.
- **X-Ray:**
    - Tracing and visual analysis of your application.
    - A picture of the state of the application.
    - Pinpoint service issues.
    - Review request behavior.
- **Service health dashboard:**
    - AWS status.
    - HEalth of the services for all regions.
    - Historical information for each day.
    - High level overview of all the services.
- **Personal health dashboard:**
    - Alerts and remediation guidance when AWS is experienciing events that may impact you.
    - Relevant and timely information to help you manage events.
## VPC and networking:
- Virtual Private Cloud: private network to deploy your resources (regional resource).
- Subnets: allow you to partition  your network inside your VPC.
- Public subnet accesible from the internet.
- Route tables. EC2 instances are created on a public subnet.
- Databases on the private subnet.
- Internet gateways helps our VPC instances connect with the internet.
- NAT gateways allow your instances in your private subnets to access the internet while remaining private.
- EC2 instances created on our default VPC go in the range of VPC -> IPv4 CIDR.
- Subnets partition of our VPC.
- Internet gateway attach to VPC (route table).
- NAT gateway associated to a subnet to access internet.
- **Network ACL and security groups:**
    - NACL:
        - Operates at subnet level.
        - Firewall which controls traffic from a to subnet.
        - Attached at a subnet level.
        - Rules only include IP addresses (ALLOW or DENY).
        - Stateless.
    - Security groups:
        - Stateful.
        - Operates at instance level.
        - Firewall that controls traffic to and from an EC2 instance.
        - Can have only ALLOW rules.
        - Rules include IP addresses and other security groups.
- **VPC flow logs:**
    - Capture information about IP traffic going into your interfaces (VPC, subnet, elastic network interface).
- **VPC peering:**
    - COnnect two VPC, privately using AWS network.
    - Make them behave as if the were in the same network.
    - Connection is not transitive (must be established for each VPC that need to commnicate with one another).
- **VPC Endpoints - Interface & Gateway (S3 and DynamoDB)**
    - Allows to connect to AWS services using a private network instead of the public www network.
    - Enhanced security and lower latency.
    - VPC Endpoint gateway.
    - Amazon S3 Dynamo DB (gateway). For the rest of services is interface.
- **Direct connect and site-to-site VPN:**
    - Direct connect:
        - Physical connection between on-premises and AWS.
        - COnnection is private, secure and fast.
        - At least one month to establish.
        - More expensive.
    - Site-to-site VPN:
        -  On-premises VPN to AWS.
        - Connection is automatically encrypted.
        - Goes over the public internet.
        - Limited bandwidth.
        - Customer Gateway (CGW).
        - Must use a Virtual Private Gateway (VGW).
- **Transit gateway overview:**
    - Peering connection between thousands of VPC and on-premises.
    - One single gateway.

## AWS shared responsbility model:
- **AWS responsability:**
    - Protecting infraestructure.
    - Managed service like S3, DynamoDB, RDS, etc.
    - Customer responsability:
        - Security in the cloud.
        - Encrypting application data.
    - Shared:
        - Patch managgement, configuration management, awareness and training.
- **RDS:**
    - AWS:
        - Manage the EC2 instance, disable SSH access.
        - Automated DB patching.
        - Automated OS patching.
    - Customer:
        - Check the ports /IP/ security groups in DB SG.
        - In-database user creation.
        - Creating a database with or withoud internet access.
- **S3:**
    - AWS:
        - Guarantee unlimited storage.
        - Guarantee you get encryption.
        - Ensure preparation of the data between customers.
        - Ensure AWS employees cannot access your data.
    - Customer:
        - Bucket configuration.
        - Bucket policy / public settings.
        - IAM users and rules.
        - Enabling encryption.
- **DDos protection:**
    - Distributed denial of service attack.
    - Use AWS shield standard.
        - Free service activated for every AWS customer.
        - Layer 3 4 attacks.
    - Use AWS shield advanced (premium).
        - $ 3000 protects against more sophisticated attacks.
        - 24/7 access to AWS DDoS response attack team.
    - AWS WAF to filter specific requests based on rules.
        - Layer 7 (common web exploits).
        -  Deploy on AApplication load balancer, API gateways, CloudFront.
        - Define Web ACL. Rules can include IP addresses, HTTP headers, HTTP body or URI strings.
        - Size constraints, geo-match.
        - Rate-based rules (to count occurences of events).
    - CloudFront and Route 53:
        - Availability protection using global edge network.
        - Combined with AWS shield provides attack mitigation at the edge.
    - Penetration testing on AWS cloud:
        - Penetration testing without prior approval for 8 services (EC2, RDS, NAT gateways, Cloudfront, AUrora, API gateways, AWS lambda, etc).
        - Contact aws-security-simulated-event@amazon.com
- **Encryption with KMS and cloudHSM:**
    - Data at rest:
        - data stored or archived on a device.
    - Data in transit:
        - data being moved from one location to another (to AWS, EC2 to DynamoDB, etc).
    - Encrypt data in both sites to protected. Leverage encryption keys.
    - **Key management services:** AWS manages the encryption keys for us.
        - Encryption opt-in (EBS volumes, S3 buckets, Redshift, rds db).
        - **Hands-on:**
            - To verify the keys go to Key Management Service (KMS) -> managed keys.
            - EC2 -> volumes -> create volume (1 GB) -> Encryption -> aws/ebs (or your own key, costs money) -> Create volume.
            - Cloudtrail is encryption enabled by default.
            - Custome managed keys (costs 1 dollar). It has key rotation.
    - **CloudHSM:**
        - AWS provisions encryption hardware.    
        - Hardware security module.
        - You manage your own encryption keys entirely (not AWS).
        - Types of customer master keys (CMK):
            - Customer managed CMK:
                - Create, manage and ussed by the customer, can enable or disable.
                - Rotation policy.
            - AWS managed CMK:
                - Created, managed and used on the customer's behalf by AWS.
                - Used by AWS services (aws/s3, aws ebs, aws redshift).
            - AWS owned CMK:
                - Collection of CMKs that an AWS service owns and manages to use in multiple accounts.
            - CloudHSM keys:
                - Keys generated from your own CloudHSM.
    
- **AWS secrets manager:**
    - Meant for storing secrets.
    - Force rotation of secrets every X days.
    - Integration with Amazon RDS (relational dbs).
    - Secrets are encrypted with KMS.
- **AWS artifact:**
    - Portal that provides customers with on-demand access to AWS compliance documentation and AWS agreements.
    - Artifact reports.
- **GuardDuty:**
    - Intelligent threat discovery to protect aws account.
    - Uses machine learning algorithms, anomaly detection, 3rd party data.
    - Read data from various sources.
    - You can setup cloudwatch event to be notified in case of findings.
    - It can trigger a lambda function or send SNS notification.
- **Amazon inspector:**
    - Automated security assessments for EC2 instances.
    - Analyze the running OS against known vulnerabilities, against unintended network accessibility.
    - Must be installed on OS in EC2 instances.
    - You get a report at the end.
- **AWS config:** 
    - Helps auditing and recording compliance of your AWS resources.
    - Possibility of storing the configuration data into S3.
    - Record configurations and changes over time.
- **Macle:**
    - 